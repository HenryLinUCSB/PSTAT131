---
title: "Homework 6"
author: "PSTAT 131/231"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Tree-Based Models

For this assignment, we will continue working with the file `"pokemon.csv"`, found in `/data`. The file is from Kaggle: <https://www.kaggle.com/abcsds/pokemon>.

The [Pokémon](https://www.pokemon.com/us/) franchise encompasses video games, TV shows, movies, books, and a card game. This data set was drawn from the video game series and contains statistics about 721 Pokémon, or "pocket monsters." In Pokémon games, the user plays as a trainer who collects, trades, and battles Pokémon to (a) collect all the Pokémon and (b) become the champion Pokémon trainer.

Each Pokémon has a [primary type](https://bulbapedia.bulbagarden.net/wiki/Type) (some even have secondary types). Based on their type, a Pokémon is strong against some types, and vulnerable to others. (Think rock, paper, scissors.) A Fire-type Pokémon, for example, is vulnerable to Water-type Pokémon, but strong against Grass-type.

![Fig 1. Houndoom, a Dark/Fire-type canine Pokémon from Generation II.](images/houndoom.jpg){width="200"}

The goal of this assignment is to build a statistical learning model that can predict the **primary type** of a Pokémon based on its generation, legendary status, and six battle statistics.

**Note: Fitting ensemble tree-based models can take a little while to run. Consider running your models outside of the .Rmd, storing the results, and loading them in your .Rmd to minimize time to knit.**

### Exercise 1

Read in the data and set things up as in Homework 5:
```{r}
library(tidyverse)
library(tidymodels)
library(klaR)
library(ISLR)
library(rpart.plot)
library(vip)
library(randomForest)
library(xgboost)
library(corrplot)
library(ggthemes)
library(glmnet)
library(pROC)
library(ranger)
library(ggplot2)
library(vip)
library(corrr)
library(discrim)

tidymodels_prefer()
pokemon = read.csv("~/PSTAT 131/homework-6/data/pokemon.csv")
```
```{r}
library(janitor)
cleaned_pdata = clean_names(pokemon)
cleaned_pdata
```
- Use `clean_names()`
- Filter out the rarer Pokémon types
- Convert `type_1` and `legendary` to factors
```{r}
library(janitor)
pokemon_clean = clean_names(pokemon)
pokemon_clean
#Filter out the rarer Pokemon Types
pokemon_clean <- pokemon_clean %>%
  filter(type_1 == "Bug" | type_1 == "Fire"| type_1 == "Grass" | type_1 == "Normal" | type_1 == "Water" | type_1 == "Psychic")
#Convert Type_1 and legendary into factors
pokemon_clean$type_1 <- as.factor(pokemon_clean$type_1)
pokemon_clean$legendary <- as.factor(pokemon_clean$legendary)
pokemon_clean$generation <- as.factor(pokemon_clean$generation)

```

Do an initial split of the data; you can choose the percentage for splitting. Stratify on the outcome variable.
```{r}
set.seed(3435)
split_data <- initial_split(pokemon_clean, prop = .8, strata = type_1)
train_data <- training(split_data)
test_data <- testing(split_data)

dim(train_data)
dim(test_data)
```
Fold the training set using *v*-fold cross-validation, with `v = 5`. Stratify on the outcome variable.
```{r}
data_fold <- vfold_cv(train_data, v = 5, strata = type_1)
data_fold

```
Set up a recipe to predict `type_1` with `legendary`, `generation`, `sp_atk`, `attack`, `speed`, `defense`, `hp`, and `sp_def`:

- Dummy-code `legendary` and `generation`;
- Center and scale all predictors.
```{r}
# From lab 5
recipe_data <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp
                      + sp_def, data = train_data) %>%
  step_dummy(legendary) %>%
  step_dummy(generation) %>%
  step_normalize(all_predictors())

```

### Exercise 2

Create a correlation matrix of the training set, using the `corrplot` package. *Note: You can choose how to handle the continuous variables for this plot; justify your decision(s).*
```{r}
# Discussed with Abhay Zope on this problem. 
library(corrplot)
correlation_matrix <- pokemon_clean %>%
  select(-type_1)  %>%
  select(-type_2) %>%
  select(-name) %>%
  select(-generation) %>%
  select(-legendary) %>%
  correlate()
#creating the plot
rplot(correlation_matrix)
#Creating the plot
correlation_matrix %>%
  stretch() %>%
  ggplot(aes(x, y, fill = r)) +
  geom_tile() +
  geom_text(aes(label = as.character(fashion(r))))
```
What relationships, if any, do you notice? Do these relationships make sense to you?
From the correlation plot, total/attack, total/sp_atk, total/defense,total/sp_def shows strong relationships with .75, .74, and .64. I believe that this relationship makes sense to me because defense and attack plays a major role for specific pokemon's total attribute stats. 

### Exercise 3

First, set up a decision tree model and workflow. Tune the `cost_complexity` hyperparameter. Use the same levels we used in Lab 7 -- that is, `range = c(-3, -1)`. Specify that the metric we want to optimize is `roc_auc`. 
```{r}
#From lab 7
tree_specify <- decision_tree() %>%
  set_engine("rpart")
class_tree_specify <- tree_specify %>%
  set_mode("classification")
class_tree_wf <- workflow() %>%
  add_model(class_tree_specify %>% set_args(cost_complexity = tune())) %>%
  add_formula(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def)
# range c(-3, -1), from lab 7

set.seed(3435)
data_fold <- vfold_cv(train_data)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  class_tree_wf, 
  resamples = data_fold, 
  grid = param_grid, 
  metrics = metric_set(roc_auc)
)

```

Print an `autoplot()` of the results. What do you observe? Does a single decision tree perform better with a smaller or larger complexity penalty?
```{r}
autoplot(tune_res)
```
### Exercise 4

What is the `roc_auc` of your best-performing pruned decision tree on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*
```{r}
# Useing collect metrics and arrange for roc_auc
arrange(collect_metrics(tune_res, truth = 'roc_auc'))
```
The best roc_auc of the best-performing pruned decision tree on the folds is .6441646 (Preprocessor1_Model06)
### Exercise 5

Using `rpart.plot`, fit and visualize your best-performing pruned decision tree with the *training* set.

```{r}
# From lab 7
best_complexity <- select_best(tune_res)

class_tree_final <- finalize_workflow(class_tree_wf, best_complexity)

class_tree_final_fit <- fit(class_tree_final, data = train_data)

class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```
### Exercise 5

Now set up a random forest model and workflow. Use the `ranger` engine and set `importance = "impurity"`. Tune `mtry`, `trees`, and `min_n`. Using the documentation for `rand_forest()`, explain in your own words what each of these hyperparameters represent.
```{r}
# From lab 7 and also discussed with Abhay Zope
forest_model <- 
  rand_forest(
              min_n = tune(),
              mtry = tune(),
              trees = tune(),
              mode = "classification") %>% 
  set_engine("ranger") 

rand_forest_workflow <- workflow() %>% 
  add_model(forest_model) %>% 
  add_recipe(recipe_data)

```
mtry is after the split, the # of variable that is randomly selected for each split. trees is the # of trees to grow. Min_n is the minimum # of quantitative variables.

Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that `mtry` should not be smaller than 1 or larger than 8. **Explain why not. What type of model would `mtry = 8` represent?**
```{r}
params <- parameters(forest_model) %>% 
  #Note that `mtry` should not be smaller than 1 or larger than 8.
  update(mtry = mtry(range= c(1, 8)))
regular_grid <- grid_regular(params, levels = 4)
```
I believe that model would mtry = 8 would mean that all the variables would be sampled, since range is 1-8, and the 8 represents the maximum # of variables that would be randomly sampled. 
### Exercise 6

Specify `roc_auc` as a metric. Tune the model and print an `autoplot()` of the results. What do you observe? What values of the hyperparameters seem to yield the best performance?
```{r}
# I tried copying from lab 7 but I keep getting error 
#set.seed(3435)

#rf_tune <- tune_grid(
# rand_forest_workflow, 
 # resamples = data_fold, 
  #grid = regular_grid, 
  #metrics = metric_set(roc_auc)
 #)

#autoplot(rf_tune)

```

### Exercise 7

What is the `roc_auc` of your best-performing random forest model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}
#arrange(collect_metrics(new_tune, truth = 'roc_auc'))
```
### Exercise 8

Create a variable importance plot, using `vip()`, with your best-performing random forest model fit on the *training* set.

Which variables were most useful? Which were least useful? Are these results what you expected, or not?

### Exercise 9

Finally, set up a boosted tree model and workflow. Use the `xgboost` engine. Tune `trees`. Create a regular grid with 10 levels; let `trees` range from 10 to 2000. Specify `roc_auc` and again print an `autoplot()` of the results. 

What do you observe?

What is the `roc_auc` of your best-performing boosted tree model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

### Exercise 10

Display a table of the three ROC AUC values for your best-performing pruned tree, random forest, and boosted tree models. Which performed best on the folds? Select the best of the three and use `select_best()`, `finalize_workflow()`, and `fit()` to fit it to the *testing* set. 

Print the AUC value of your best-performing model on the testing set. Print the ROC curves. Finally, create and visualize a confusion matrix heat map.

Which classes was your model most accurate at predicting? Which was it worst at?

